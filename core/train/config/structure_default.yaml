MODEL:
  hidden_size: 768
  intermediate_size: 2064
  num_hidden_layers: 12
  num_attention_heads: 12
  hidden_act: silu
  max_position_embeddings: 1024
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2
  num_special_tokens: 3
OPTIMIZER:
  lr: 0.001
  wd: 0.0005
  warmup_steps: 10000
  restart_interval: 10000
  lr_decay: 0.9
  min_lr: 0.00001
  plateau_patience: 10
DATASET:
  batch_size: 8
  tube_length: 8
TRAINING:
  accumulate_grad_batches: 2
  precision: 16
  accelerator: gpu
  steps: -1  # infinite
  val_check_interval: 10000
