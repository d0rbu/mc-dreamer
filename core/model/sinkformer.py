import torch as th


# TODO: implement transformer with sink k/v tokens at each attention layer and "absolute" position embeddings